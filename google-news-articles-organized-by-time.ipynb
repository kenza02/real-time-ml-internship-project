{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-Jz6fNXkfX-",
        "outputId": "a8986eaf-7bf3-4c12-c309-cd11fa087286"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen, Request\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "from langdetect import detect  # Import the language detection function\n",
        "\n",
        "root = \"https://www.google.com/\"\n",
        "\n",
        "# Initial link for the first page\n",
        "link_base = \"https://www.google.com/search?q=tesla&rlz=1C1GCEA_enMA1047MA1047&tbas=0&tbs=qdr:d,sbd:1&tbm=nws&source=lnt&sa=X&ved=2ahUKEwiAmILcsM2AAxWyd6QEHbwdARUQpwV6BAgCEBM&biw=1366&bih=651&dpr=1\"\n",
        "articles_per_page = 10\n",
        "num_pages = 25  # Set the number of pages you want to scrape\n",
        "\n",
        "data = []\n",
        "\n",
        "# Loop through multiple pages\n",
        "for page in range(num_pages):\n",
        "    # Calculate the starting index for the current page\n",
        "    start_index = page * articles_per_page\n",
        "    link = f\"{link_base}&start={start_index}\"\n",
        "\n",
        "    req = Request(link, headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'})\n",
        "    webpage = urlopen(req).read()\n",
        "\n",
        "    # Use requests.Session() as a context manager\n",
        "    with requests.Session() as c:\n",
        "        soup = BeautifulSoup(webpage, 'html.parser')\n",
        "        articles = soup.find_all('a', class_='WlydOe')\n",
        "\n",
        "    # Loop through each article and extract time, title, and description\n",
        "    for article in articles:\n",
        "        time_element = article.find('div', class_='OSrXXb')\n",
        "        title_element = article.find('div', class_='n0jPhd')\n",
        "        description_element = article.find('div', class_='GI74Re')\n",
        "\n",
        "        if time_element:\n",
        "            time = time_element.span.get_text()\n",
        "        else:\n",
        "            time = \"N/A\"\n",
        "\n",
        "        if title_element:\n",
        "            title = title_element.get_text()\n",
        "        else:\n",
        "            title = \"N/A\"\n",
        "\n",
        "        if description_element:\n",
        "            description = description_element.get_text()\n",
        "        else:\n",
        "            description = \"N/A\"\n",
        "\n",
        "        # Use the langdetect library to detect the language of the description\n",
        "        description_language = detect(description)\n",
        "        if description_language == 'en':\n",
        "            data.append({'Time': time, 'Title': title, 'Description': description})\n",
        "\n",
        "# Create a DataFrame from the data list\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "output_file = 'tesla_articles12.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "print(f\"Data has been scraped and saved to {output_file}.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVZ-AWrloCd",
        "outputId": "b112c102-1d84-45d0-d547-8dc708615227"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been scraped and saved to tesla_articles12.xlsx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(f'start excel {output_file}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wASLoobkmDJe",
        "outputId": "4486f40a-9cfb-4e7f-d798-bbca619a429a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32512"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "output_file = '/content/tesla_articles12.xlsx'  # Remplacez ceci par le chemin de votre fichier Excel\n",
        "\n",
        "if os.path.exists(output_file):\n",
        "    os.system(f'start excel \"{output_file}\"')\n",
        "else:\n",
        "    print(f'Le fichier {output_file} n\\'existe pas.')\n"
      ],
      "metadata": {
        "id": "iJLdhNiN0DPz"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}