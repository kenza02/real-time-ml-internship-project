{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "289c3257-e923-44b3-b3cb-e11ee03688b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved to tesla_articles1.xlsx.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "root = \"https://www.google.com/\"\n",
    "\n",
    "# Initial link for the first page\n",
    "link_base = \"https://www.google.fr/search?q=Tesla&tbm=nws&source=lnt&tbs=qdr:d&sa=X&biw=1366&bih=619&dpr=1\"\n",
    "articles_per_page = 10\n",
    "num_pages = 20  # Set the number of pages you want to scrape\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop through multiple pages\n",
    "for page in range(num_pages):\n",
    "    # Calculate the starting index for the current page\n",
    "    start_index = page * articles_per_page\n",
    "    link = f\"{link_base}&start={start_index}\"\n",
    "    \n",
    "    req = Request(link, headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'})\n",
    "    webpage = urlopen(req).read()\n",
    "    \n",
    "    # Use requests.Session() as a context manager\n",
    "    with requests.Session() as c:\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        articles = soup.find_all('a', class_='WlydOe')\n",
    "    \n",
    "    # Loop through each article and extract time, title, and description\n",
    "    for article in articles:\n",
    "        time_element = article.find('div', class_='OSrXXb')\n",
    "        title_element = article.find('div', class_='n0jPhd')\n",
    "        description_element = article.find('div', class_='GI74Re')\n",
    "        \n",
    "        if time_element:\n",
    "            time = time_element.span.get_text()\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "        \n",
    "        if title_element:\n",
    "            title = title_element.get_text()\n",
    "        else:\n",
    "            title = \"N/A\"\n",
    "        \n",
    "        if description_element:\n",
    "            description = description_element.get_text()\n",
    "        else:\n",
    "            description = \"N/A\"\n",
    "        \n",
    "        data.append({'Time': time, 'Title': title, 'Description': description})\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_file = 'tesla_articles1.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "print(f\"Data has been scraped and saved to {output_file}.\")\n",
    "os.system(f'start excel {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb8275-8f96-44a6-a914-15667bc8ce84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851d5b6-4883-4437-bed5-1b555744d48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
