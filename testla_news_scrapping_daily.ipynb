{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea7484-263f-40cc-b9dd-eb6abb9316de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import schedule\n",
    "import time\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_and_save():\n",
    "    finviz_url = 'https://finviz.com/quote.ashx?t=TSLA'\n",
    "    output_file = 'tesla_articles.csv'\n",
    "    \n",
    "    # Check if the CSV file exists, if not, create it\n",
    "    if not os.path.exists(output_file):\n",
    "        df_empty = pd.DataFrame(columns=['date', 'time', 'title', 'url'])\n",
    "        df_empty.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Read existing data from the CSV file\n",
    "    df_existing = pd.read_csv(output_file)\n",
    "    existing_urls = set(df_existing['url'])\n",
    "    \n",
    "    news_tables = {}\n",
    "    \n",
    "    url = finviz_url \n",
    "    \n",
    "    req = Request(url=url, headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'})\n",
    "    response = urlopen(req)\n",
    "    \n",
    "    html = BeautifulSoup(response, features='html.parser')\n",
    "    news_table = html.find(id='news-table')\n",
    "    \n",
    "    parsed_data = []\n",
    "    \n",
    "    for row in news_table.findAll('tr'):\n",
    "        title = row.a.text\n",
    "        date_time_cell = row.find('td', align='right')  # Find the cell containing date and time\n",
    "        \n",
    "        if date_time_cell:\n",
    "            date_time_text = date_time_cell.get_text(strip=True)\n",
    "            date_time_split = date_time_text.split(' ')\n",
    "            \n",
    "            if len(date_time_split) == 2:\n",
    "                date = date_time_split[0]\n",
    "                time = date_time_split[1]\n",
    "            else:\n",
    "                date = None\n",
    "                time = date_time_text\n",
    "        else:\n",
    "            date = None\n",
    "            time = None\n",
    "        \n",
    "        article_url = row.a['href']\n",
    "        \n",
    "        # Check if the article URL is new, if yes, add it to the parsed_data\n",
    "        if article_url not in existing_urls:\n",
    "            parsed_data.append([date, time, title, article_url])\n",
    "            existing_urls.add(article_url)\n",
    "    \n",
    "    if parsed_data:\n",
    "        df_new = pd.DataFrame(parsed_data, columns=['date', 'time', 'title', 'url'])\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(output_file, index=False)\n",
    "        print(f\"New data has been scraped and saved to {output_file}.\")\n",
    "    else:\n",
    "        print(\"No new articles found.\")\n",
    "\n",
    "# Schedule the scraping to run daily at a specific time\n",
    "schedule.every().day.at(\"08:00\").do(scrape_and_save)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590ad0c-ab4a-4023-a861-792cd59dbb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
